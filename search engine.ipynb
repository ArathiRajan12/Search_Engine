{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f083fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d57b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs_and_clean():\n",
    "    r = requests.get('https://bola.kompas.com/')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    link = []\n",
    "    for i in soup.find('div', {'class':'most__wrap'}).find_all('a'):\n",
    "        i['href'] = i['href'] + '?page=all'\n",
    "        link.append(i['href'])\n",
    "    documents = []\n",
    "\n",
    "    for i in link:\n",
    "        r = requests.get(i)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        sen = []\n",
    "        for i in soup.find('div', {'class':'read__content'}).find_all('p'):\n",
    "            sen.append(i.text)\n",
    "        documents.append(' '.join(sen))\n",
    "        documents_clean = []\n",
    "    for d in documents:\n",
    "        document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "        document_test = re.sub(r'@\\w+', '', document_test)\n",
    "        document_test = document_test.lower()\n",
    "        document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "        document_test = re.sub(r'[0-9]', '', document_test)\n",
    "        document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "        documents_clean.append(document_test)\n",
    "\n",
    "    return documents_clean\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a940a445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0         1         2         3         4         5  \\\n",
      "acapkali  0.000000  0.000000  0.000000  0.000000  0.000000  0.037931   \n",
      "ada       0.048771  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "adalah    0.027966  0.000000  0.000000  0.017061  0.033046  0.000000   \n",
      "adapun    0.075871  0.088384  0.038426  0.000000  0.000000  0.000000   \n",
      "adu       0.000000  0.000000  0.000000  0.000000  0.000000  0.037931   \n",
      "\n",
      "                 6         7         8         9  \n",
      "acapkali  0.000000  0.000000  0.000000  0.000000  \n",
      "ada       0.000000  0.000000  0.034168  0.000000  \n",
      "adalah    0.014312  0.057672  0.039185  0.023965  \n",
      "adapun    0.000000  0.026077  0.000000  0.000000  \n",
      "adu       0.000000  0.000000  0.000000  0.000000  \n",
      "(924, 10)\n"
     ]
    }
   ],
   "source": [
    "docs = retrieve_docs_and_clean()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "#Create a dataframe\n",
    "df = pd.DataFrame(X.T.toarray(), index=vectorizer.get_feature_names())\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c46b6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acapkali</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034168</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adalah</th>\n",
       "      <td>0.027966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017061</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>0.057672</td>\n",
       "      <td>0.039185</td>\n",
       "      <td>0.023965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adapun</th>\n",
       "      <td>0.075871</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adu</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5  \\\n",
       "acapkali  0.000000  0.000000  0.000000  0.000000  0.000000  0.037931   \n",
       "ada       0.048771  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "adalah    0.027966  0.000000  0.000000  0.017061  0.033046  0.000000   \n",
       "adapun    0.075871  0.088384  0.038426  0.000000  0.000000  0.000000   \n",
       "adu       0.000000  0.000000  0.000000  0.000000  0.000000  0.037931   \n",
       "\n",
       "                 6         7         8         9  \n",
       "acapkali  0.000000  0.000000  0.000000  0.000000  \n",
       "ada       0.000000  0.000000  0.034168  0.000000  \n",
       "adalah    0.014312  0.057672  0.039185  0.023965  \n",
       "adapun    0.000000  0.026077  0.000000  0.000000  \n",
       "adu       0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retrieve_docs_and_clean()\n",
    "# Create Term-Document Matrix with TF-IDF weighting\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(X.T.toarray(), index=vectorizer.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c464ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: barcelona\n",
      "Berikut artikel dengan nilai cosine similarity tertinggi: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: gareth bale\n",
      "Berikut artikel dengan nilai cosine similarity tertinggi: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: shin tae yong\n",
      "Berikut artikel dengan nilai cosine similarity tertinggi: \n"
     ]
    }
   ],
   "source": [
    "def get_similar_articles(q, df):\n",
    "    print(\"query:\", q)\n",
    "    print(\"Berikut artikel dengan nilai cosine similarity tertinggi: \")\n",
    "    q = [q]\n",
    "    q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    sim = {}\n",
    "    for i in range(10):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "  \n",
    "    sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "  \n",
    "    for k, v in sim_sorted:\n",
    "        if v != 0.0:\n",
    "            print(\"Nilai Similaritas:\", v)\n",
    "            print(docs[k])\n",
    "            print()\n",
    "\n",
    "q1 = 'barcelona'\n",
    "q2 = 'gareth bale'\n",
    "q3 = 'shin tae yong'\n",
    "\n",
    "get_similar_articles(q1, df)\n",
    "print('-'*100)\n",
    "get_similar_articles(q2, df)\n",
    "print('-'*100)\n",
    "get_similar_articles(q3, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
